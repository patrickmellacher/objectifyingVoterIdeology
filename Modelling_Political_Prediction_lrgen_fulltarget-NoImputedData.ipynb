{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257c8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, RandomOverSampler, SMOTE\n",
    "from imblearn import FunctionSampler  # to use a idendity sampler\n",
    "\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7c2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "IMPUTATION_REQUIRED=0\n",
    "TRAIN_SIZE=0.8\n",
    "PAR_NORMALIZE=0\n",
    "PAR_SCALE=1\n",
    "target_col = 'lrgen' # 'lrgen', 'lrecon', 'galtan'\n",
    "target_factor = target_col + '_factor' # 'lrgen_factor', 'lrecon_factor', 'galtan_factor'\n",
    "# check if data is imputed or not\n",
    "IMPUTED_DATA = 'No' # 'Yes' if imputed\n",
    "SOURCE_DATA_FILES = 'data/base_data/' #'data/imputed_data/recategorized/imputation_cart/'\n",
    "FILE_SUFFIX = '' #'_recategorized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3b9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_raw = pd.read_csv(\"data/CHES2019_experts_imputed_cart.csv\", index_col=0).reset_index(drop=True)\n",
    "data_X = pd.read_csv(SOURCE_DATA_FILES + 'data_' + target_col + '_base_X_train' + FILE_SUFFIX + '.csv', index_col = 0)\n",
    "data_y = pd.read_csv(SOURCE_DATA_FILES + 'data_' + target_col + '_base_y_train' + FILE_SUFFIX + '.csv', index_col = 0)\n",
    "if IMPUTED_DATA == 'Yes':\n",
    "    valid_X = pd.read_csv(SOURCE_DATA_FILES + 'data_' + target_col + '_base_X_valid' + FILE_SUFFIX + '.csv', index_col = 0)\n",
    "    valid_y = pd.read_csv(SOURCE_DATA_FILES + 'data_' + target_col + '_base_y_valid' + FILE_SUFFIX + '.csv', index_col = 0)\n",
    "test_X = pd.read_csv(SOURCE_DATA_FILES + 'data_' + target_col + '_base_X_test' + FILE_SUFFIX + '.csv', index_col = 0)\n",
    "test_y = pd.read_csv(SOURCE_DATA_FILES + 'data_' + target_col + '_base_y_test' + FILE_SUFFIX + '.csv', index_col = 0)\n",
    "#data = pd.read_csv(\"data/CHES2019_experts_imputed_pmm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12364fd",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "Preprocessing steps (after imputation in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872cae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test dataset\n",
    "# split data for obtaining a separated TEST dataset (not used for training)\n",
    "X_train = data_X\n",
    "X_test = test_X\n",
    "y_train = data_y\n",
    "y_test = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9e2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization/normalization of data\n",
    "# Be aware: information related to scaling MUST NOT flow into the separated datasets\n",
    "# --> 1st split, 2nd scaling\n",
    "if (PAR_NORMALIZE==1):\n",
    "    data_normalized = preprocessing.normalize(X_train)\n",
    "\n",
    "if (PAR_SCALE==1):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    data_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b6e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_base = X_train\n",
    "y_train_base = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c76336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrgen_factor\n",
       "7.0             348\n",
       "5.0             339\n",
       "6.0             319\n",
       "4.0             310\n",
       "8.0             278\n",
       "3.0             276\n",
       "2.0             242\n",
       "9.0             170\n",
       "1.0             135\n",
       "10.0            117\n",
       "0.0              59\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7375d6e",
   "metadata": {},
   "source": [
    "# Oversampling to balance the dataset regarding target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df5c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samplers = [\n",
    "#    FunctionSampler(),\n",
    "#    RandomOverSampler(random_state=0),\n",
    "#    SMOTE(random_state=0),\n",
    "#    ADASYN(random_state=0),\n",
    "#]\n",
    "\n",
    "\n",
    "#X_train, y_train = RandomOverSampler(random_state=0).fit_resample(X_train_base, y_train_base)\n",
    "\n",
    "\n",
    "###\n",
    "X_train, y_train = SMOTE(random_state=0, sampling_strategy='not majority').fit_resample(X_train_base, y_train_base)\n",
    "###\n",
    "\n",
    "\n",
    "#X_train, y_train = ADASYN(random_state=0, sampling_strategy='minority').fit_resample(X_train_base, y_train_base)\n",
    "\n",
    "\n",
    "#for sampler in samplers:\n",
    "#    sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c8ce0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrgen_factor\n",
       "0.0             348\n",
       "1.0             348\n",
       "2.0             348\n",
       "3.0             348\n",
       "4.0             348\n",
       "5.0             348\n",
       "6.0             348\n",
       "7.0             348\n",
       "8.0             348\n",
       "9.0             348\n",
       "10.0            348\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cd4b7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>econ_interven</th>\n",
       "      <th>environment</th>\n",
       "      <th>redistribution</th>\n",
       "      <th>civlib_laworder</th>\n",
       "      <th>immigrate_policy</th>\n",
       "      <th>sociallifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3828.000000</td>\n",
       "      <td>3828.000000</td>\n",
       "      <td>3828.000000</td>\n",
       "      <td>3828.000000</td>\n",
       "      <td>3828.000000</td>\n",
       "      <td>3828.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.098164</td>\n",
       "      <td>5.201075</td>\n",
       "      <td>4.035788</td>\n",
       "      <td>5.169940</td>\n",
       "      <td>5.592525</td>\n",
       "      <td>4.435826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.896049</td>\n",
       "      <td>2.813948</td>\n",
       "      <td>2.799683</td>\n",
       "      <td>3.204217</td>\n",
       "      <td>3.190781</td>\n",
       "      <td>3.474790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.863918</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.109209</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       econ_interven  environment  redistribution  civlib_laworder  \\\n",
       "count    3828.000000  3828.000000     3828.000000      3828.000000   \n",
       "mean        4.098164     5.201075        4.035788         5.169940   \n",
       "std         2.896049     2.813948        2.799683         3.204217   \n",
       "min         0.000000     0.000000        0.000000         0.000000   \n",
       "25%         1.863918     3.000000        2.000000         2.000000   \n",
       "50%         4.000000     5.000000        4.000000         5.000000   \n",
       "75%         6.000000     7.109209        6.000000         8.000000   \n",
       "max        10.000000    10.000000       10.000000        10.000000   \n",
       "\n",
       "       immigrate_policy  sociallifestyle  \n",
       "count       3828.000000      3828.000000  \n",
       "mean           5.592525         4.435826  \n",
       "std            3.190781         3.474790  \n",
       "min            0.000000         0.000000  \n",
       "25%            3.000000         1.000000  \n",
       "50%            5.000000         4.000000  \n",
       "75%            9.000000         8.000000  \n",
       "max           10.000000        10.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac093d8",
   "metadata": {},
   "source": [
    "# Principal Component Analysis: reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30692ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PCA with 2 components\n",
    "# combine the result with the target result\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X_train)\n",
    "principalDf = pd.DataFrame(data = principalComponents,\n",
    "                           columns = ['PC1', \n",
    "                                        'PC2'],\n",
    "                          index=X_train.index)\n",
    "finalDf = pd.concat([principalDf, y_train], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.noise_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22711db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf[target_factor].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf882a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8146590",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c49ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results of the PCA\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC1', fontsize = 15)\n",
    "ax.set_ylabel('PC2', fontsize = 15)\n",
    "ax.set_title('PCA LRGEN', fontsize = 20)\n",
    "\n",
    "targets = range(0,11)\n",
    "colors = ['r', 'g', 'b', 'c',  'm',  'y',  'k',  'gray',  'lime',  'orange',  'gold']\n",
    "for target, color in zip(targets,colors):\n",
    "    points = finalDf[finalDf[target_factor] == target][['PC1', 'PC2']].values\n",
    "    # get convex hull\n",
    "    hull = ConvexHull(points)\n",
    "    # get x and y coordinates\n",
    "    # repeat last point to close the polygon\n",
    "    x_hull = np.append(points[hull.vertices,0],\n",
    "                       points[hull.vertices,0][0])\n",
    "    y_hull = np.append(points[hull.vertices,1],\n",
    "                       points[hull.vertices,1][0])\n",
    "    # plot shape\n",
    "    plt.fill(x_hull, y_hull, alpha=0.3, c=color)\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf[target_factor] == target #indicesToKeep = finalDf[target_factor].factorize()[0] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PC1']\n",
    "               , finalDf.loc[indicesToKeep, 'PC2']\n",
    "               , c = color\n",
    "               , s = 10)\n",
    "\n",
    "\n",
    "    \n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "#plt.savefig('pics/PCA_LRGEN_FULL.png', dpi=600)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f047c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results of the PCA\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC1', fontsize = 15)\n",
    "ax.set_ylabel('PC2', fontsize = 15)\n",
    "ax.set_title('PCA LRGEN', fontsize = 20)\n",
    "\n",
    "targets = range(0,11)\n",
    "colors = ['r', 'g', 'b', 'c',  'm',  'y',  'k',  'gray',  'lime',  'orange',  'gold']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf[target_factor] == target #indicesToKeep = finalDf[target_factor].factorize()[0] == target\n",
    "    x = finalDf.loc[indicesToKeep, 'PC1']\n",
    "    y = finalDf.loc[indicesToKeep, 'PC2']\n",
    "    ax.scatter(x,\n",
    "               y,\n",
    "               c = color,\n",
    "               s = 10)\n",
    "    \n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf[target_factor] == target #indicesToKeep = finalDf[target_factor].factorize()[0] == target\n",
    "    x = finalDf.loc[indicesToKeep, 'PC1']\n",
    "    y = finalDf.loc[indicesToKeep, 'PC2']\n",
    "    ax.scatter(sum(x) / len(points), sum(y) / len(points), c=color, s=500, marker='^')\n",
    "    print(sum(x) / len(points), sum(y) / len(points))\n",
    "    \n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "#plt.savefig('pics/PCA_LRGEN_FULL_low_resolution.png', dpi=150)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 80% of the variance are explained through the 2 components\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)\n",
    "print(pca.feature_names_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40adfc0c",
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(random_state=17)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "y=y_train\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(\n",
    "    X_tsne[:, 0],\n",
    "    X_tsne[:, 1],\n",
    "    c=y['lrgen_factor'].values,\n",
    "    edgecolor=\"none\",\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    cmap=plt.cm.get_cmap(\"nipy_spectral\", 10),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.title(\"MNIST. t-SNE projection\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c5ee7",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Check if some patterns can be observed using unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed425d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to obtain best number of clusters through scree plot\n",
    "distortions = []\n",
    "for i in range(1, 15):\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(finalDf[['PC1', 'PC2']])\n",
    "    distortions.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 15), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to cluster results of PCA\n",
    "km = KMeans(\n",
    "    n_clusters=11, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(finalDf[['PC1', 'PC2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e334e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11aaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = pd.concat([finalDf, \n",
    "                          pd.DataFrame(y_km, \n",
    "                                       index=X_train.index, \n",
    "                                       columns=['cluster'])], \n",
    "                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data.head(20)\n",
    "#cluster_data[cluster_data['lrgen_factor']==10]['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data.pivot_table(aggfunc='count', columns = [target_factor, 'cluster']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5d4a8",
   "metadata": {},
   "source": [
    "# Non-negative matrix Factorization\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a NMF with 2 components\n",
    "# combine the result with the target result\n",
    "nmf = NMF(n_components=2, random_state = 36, max_iter=1000)\n",
    "nmf_matrices = nmf.fit_transform(X_train)\n",
    "nmfDf = pd.DataFrame(data = nmf_matrices,\n",
    "                           columns = ['PC1', \n",
    "                                        'PC2'],\n",
    "                          index=X_train.index\n",
    "                    )\n",
    "finalnmfDf = pd.concat([nmfDf, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70085091",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalnmfDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd216e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab398a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results of the NMF\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC1', fontsize = 15)\n",
    "ax.set_ylabel('PC2', fontsize = 15)\n",
    "ax.set_title('NMF LRGEN', fontsize = 20)\n",
    "\n",
    "targets = range(0,12)\n",
    "colors = ['r', 'g', 'b', 'c',  'm',  'y',  'k',  'gray',  'lime',  'orange',  'gold']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalnmfDf[target_factor] == target #indicesToKeep = finalDf[target_factor].factorize()[0] == target\n",
    "    ax.scatter(finalnmfDf.loc[indicesToKeep, 'PC1']\n",
    "               , finalnmfDf.loc[indicesToKeep, 'PC2']\n",
    "               , c = color\n",
    "               , s = 10)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "#plt.savefig('pics/PCA_LRGEN_FULL.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86255f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a NMF with 3 components\n",
    "# combine the result with the target result\n",
    "nmf = NMF(n_components=3, random_state = 36, max_iter=1000)\n",
    "nmf_matrices = nmf.fit_transform(X_train)\n",
    "nmfDf = pd.DataFrame(data = nmf_matrices,\n",
    "                           columns = ['PC1', \n",
    "                                        'PC2', 'PC3']#,\n",
    "                          #index=X_train.index\n",
    "                    )\n",
    "finalnmfDf = pd.concat([nmfDf, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, finalnmfDf[['PC1', 'PC2', 'PC3']], np.ravel(finalnmfDf['lrgen_factor']), cv=5)\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineSVC = make_pipeline(StandardScaler(), SVC(random_state=1))\n",
    "# Create the parameter grid\n",
    "param_grid_svc = [{\n",
    "                    'svc__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__kernel': ['linear'],\n",
    "                    'svc__degree': [1,2,3,4,5]\n",
    "                  },\n",
    "                 {\n",
    "                    'svc__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__gamma': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__kernel': ['rbf']\n",
    "                 }]\n",
    "# Create an instance of GridSearch Cross-validation estimator\n",
    "gsSVC = GridSearchCV(estimator=pipelineSVC,\n",
    "                     param_grid = param_grid_svc,\n",
    "                     scoring=scoring_method,\n",
    "                     cv=cv_nr,\n",
    "                     refit=True,\n",
    "                     n_jobs=1)\n",
    "# Train the SVM classifier\n",
    "gsSVC.fit(finalnmfDf[['PC1', 'PC2', 'PC3']], np.ravel(finalnmfDf['lrgen_factor']))\n",
    "# Print the training score of the best model\n",
    "print(gsSVC.best_score_)\n",
    "# Print the model parameters of the best model\n",
    "print(gsSVC.best_params_)\n",
    "# Print the model score on the test data using GridSearchCV score method\n",
    "print('Test accuracy: %.3f' % gsSVC.score(nmf.transform(X_test), y_test))\n",
    "# Print the model score on the test data using Best estimator instance\n",
    "clfSVCnmf = gsSVC.best_estimator_\n",
    "#print('Test accuracy: %.3f' % clfSVC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb2a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "407cf8cc",
   "metadata": {},
   "source": [
    "# Supervised learning: various algorithms\n",
    "### Sources: https://vitalflux.com/grid-search-explained-python-sklearn-examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2e45b",
   "metadata": {},
   "source": [
    "## 1) Support Vector Classification\n",
    "\n",
    "CHECK: RMSE and other scoring mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### PARAMETERS \n",
    "scoring_method = 'balanced_accuracy'#['balanced_accuracy', 'f1_samples', 'roc_auc_ovo_weighted']\n",
    "cv_nr = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbdbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS-VALIDATION (without TEST dataset)\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X_train, np.ravel(y_train), cv=5)\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a464e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineSVC = make_pipeline(StandardScaler(), SVC(random_state=1))\n",
    "# Create the parameter grid\n",
    "param_grid_svc = [{\n",
    "                    'svc__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__kernel': ['linear'],\n",
    "                    'svc__degree': [1,2,3,4,5]\n",
    "                  },\n",
    "                 {\n",
    "                    'svc__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__gamma': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__kernel': ['rbf']\n",
    "                 }]\n",
    "# Create an instance of GridSearch Cross-validation estimator\n",
    "gsSVC = GridSearchCV(estimator=pipelineSVC,\n",
    "                     param_grid = param_grid_svc,\n",
    "                     scoring=scoring_method,\n",
    "                     cv=cv_nr,\n",
    "                     refit=True,\n",
    "                     n_jobs=1)\n",
    "# Train the SVM classifier\n",
    "gsSVC.fit(X_train, np.ravel(y_train))\n",
    "# Print the training score of the best model\n",
    "print(gsSVC.best_score_)\n",
    "# Print the model parameters of the best model\n",
    "print(gsSVC.best_params_)\n",
    "# Print the model score on the test data using GridSearchCV score method\n",
    "print('Test accuracy: %.3f' % gsSVC.score(X_test, y_test))\n",
    "# Print the model score on the test data using Best estimator instance\n",
    "clfSVC = gsSVC.best_estimator_\n",
    "#print('Test accuracy: %.3f' % clfSVC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0e9d0",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineRFC = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=1, criterion='entropy')) \n",
    "# Create the parameter grid\n",
    "                            \n",
    "param_grid_rfc = [{\n",
    "    'randomforestclassifier__max_depth':[2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'randomforestclassifier__max_features':[2, 3, 4, 5, 6],\n",
    "    'randomforestclassifier__n_estimators':range(5, 105, 5)#,\n",
    "    #'randomforestclassifier__criterion:':['gini', 'entropy']\n",
    "}]\n",
    "# Create an instance of GridSearch Cross-validation estimator\n",
    "gsRFC = GridSearchCV(estimator=pipelineRFC,\n",
    "                     param_grid = param_grid_rfc,\n",
    "                     scoring=scoring_method,\n",
    "                     cv=cv_nr,\n",
    "                     refit=True,\n",
    "                     n_jobs=1)\n",
    "# Train the RandomForestClassifier\n",
    "gsRFC = gsRFC.fit(X_train, np.ravel(y_train))\n",
    "# Print the training score of the best model\n",
    "print(gsRFC.best_score_)\n",
    "# Print the model parameters of the best model\n",
    "print(gsRFC.best_params_)\n",
    "# Print the test score of the best model\n",
    "clfRFC = gsRFC.best_estimator_\n",
    "print('Test accuracy: %.3f' % clfRFC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db57429",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858a9df",
   "metadata": {},
   "source": [
    "## Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af846e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineLR = make_pipeline(StandardScaler(), LogisticRegression(random_state=1, penalty='l2', solver='lbfgs'))\n",
    "# Create the parameter grid\n",
    "param_grid_lr = [{\n",
    "    'logisticregression__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "}]\n",
    "# Create an instance of GridSearch Cross-validation estimator\n",
    "gsLR = GridSearchCV(estimator=pipelineLR,\n",
    "                     param_grid = param_grid_lr,\n",
    "                     scoring=scoring_method,\n",
    "                     cv=cv_nr,\n",
    "                     refit=True,\n",
    "                     n_jobs=1)\n",
    "# Train the LogisticRegression Classifier\n",
    "gsLR = gsLR.fit(X_train, np.ravel(y_train))\n",
    "# Print the training score of the best model\n",
    "print(gsLR.best_score_)\n",
    "# Print the model parameters of the best model\n",
    "print(gsLR.best_params_)\n",
    "# Print the test score of the best model\n",
    "clfLR = gsLR.best_estimator_\n",
    "print('Test accuracy: %.3f' % clfLR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d70c9",
   "metadata": {},
   "source": [
    "## ADABoost Classification (Ensemble Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineADAB = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=1))\n",
    "# Create the parameter grid\n",
    "param_grid_ADAB = [{\n",
    "    'adaboostclassifier__n_estimators': [2, 10, 20, 30, 40, 50, 100]\n",
    "}]\n",
    "# Create an instance of GridSearch Cross-validation estimator\n",
    "gsADAB = GridSearchCV(estimator=pipelineADAB,\n",
    "                     param_grid = param_grid_ADAB,\n",
    "                     scoring=scoring_method,\n",
    "                     cv=cv_nr,\n",
    "                     refit=True,\n",
    "                     n_jobs=1)\n",
    "# Train the LogisticRegression Classifier\n",
    "gsADAB = gsADAB.fit(X_train, np.ravel(y_train))\n",
    "# Print the training score of the best model\n",
    "print(gsADAB.best_score_)\n",
    "# Print the model parameters of the best model\n",
    "print(gsADAB.best_params_)\n",
    "# Print the test score of the best model\n",
    "clfADAB = gsADAB.best_estimator_\n",
    "print('Test accuracy: %.3f' % clfADAB.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee72496",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsADAB.scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497d0d9",
   "metadata": {},
   "source": [
    "# Test models with independent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = preprocessing.StandardScaler().fit(test_X)\n",
    "#test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(test_y, gsADAB.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1931ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(test_y, gsADAB.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_squared_error(test_y, gsADAB.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60675d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(test_y, gsADAB.predict(test_X), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(test_y, gsLR.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ddecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(test_y, gsLR.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8abe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_squared_error(test_y, gsLR.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96177185",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(test_y, gsLR.predict(test_X), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(test_y, gsRFC.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(test_y, gsRFC.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012eedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_squared_error(test_y, gsRFC.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(test_y, gsRFC.predict(test_X), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61907a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(test_y, gsSVC.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(test_y, gsSVC.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_squared_error(test_y, gsSVC.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ac7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(test_y, gsSVC.predict(test_X), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc78a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline = Pipeline(steps=[(\"preprocesser\", preprocessor), (\"classifier\", LogisticRegression())])\n",
    "#pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91febc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "#pipe.fit(X_train, y_train)\n",
    "#Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\n",
    "#pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a808ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write several scores to the output ###\n",
    "\n",
    "RMSE = True\n",
    "model_vector = [gsADAB, gsLR, gsRFC, gsSVC]\n",
    "for model in model_vector:\n",
    "    print(model.estimator.named_steps)\n",
    "    print('Accuracy Score: ', sklearn.metrics.accuracy_score(test_y, model.predict(test_X)))\n",
    "    print('Balanced Accuracy Score: ', sklearn.metrics.balanced_accuracy_score(test_y, model.predict(test_X)))\n",
    "    print('Cohen-Kappa-Score: ', sklearn.metrics.cohen_kappa_score(test_y, model.predict(test_X)))\n",
    "    print('F1-Score: ', sklearn.metrics.f1_score(test_y, model.predict(test_X), average='weighted'))\n",
    "    if RMSE == True:\n",
    "        print('RMSE: ', sklearn.metrics.mean_squared_error(test_y, model.predict(test_X)))\n",
    "        #print('ROC-AUC-Score: ', sklearn.metrics.roc_auc_score(test_y, \n",
    "        #                                                       model.predict(test_X), \n",
    "        #                                                       average='weighted', \n",
    "        #                                                       multi_class='ovo'))\n",
    "    print('__________________________________\\n')\n",
    "\n",
    "for model in model_vector:\n",
    "    print('Confusion matrix: \\n', sklearn.metrics.confusion_matrix(test_y, model.predict(test_X)))\n",
    "\n",
    "for model in model_vector:\n",
    "    print(sklearn.metrics.classification_report(test_y, model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd71e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features:           ', gsRFC.feature_names_in_)\n",
    "print('Feature Importance: ', gsRFC.best_estimator_[1].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(gsRFC, 'models/gsRFC_lrgen_fulltarget_NoImputation.joblib') \n",
    "#clf = load('filename.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc271ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
