{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a62b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, RandomOverSampler, SMOTE\n",
    "from imblearn import FunctionSampler  # to use a idendity sampler\n",
    "\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448f6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_relaxed(confusion_matrix_temp):\n",
    "    # returns the relative number of predictions which are\n",
    "    # +/- 1 compared to the actual result should be provided.\n",
    "    i_cnt = confusion_matrix_temp.shape[0]\n",
    "    j_cnt = confusion_matrix_temp.shape[1]\n",
    "    bin_matrix_temp = confusion_matrix_temp.copy()\n",
    "    for i in range(0, i_cnt):\n",
    "        for j in range(0, j_cnt):\n",
    "            if (abs(i-j)==1):\n",
    "                bin_matrix_temp[i,j] = abs(i-j)\n",
    "            else:\n",
    "                bin_matrix_temp[i,j] = 0\n",
    "    return ((confusion_matrix_temp*bin_matrix_temp).sum()) / (confusion_matrix_temp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "176621a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_perc(confusion_matrix_temp):\n",
    "    # returns the relative number of predictions which are\n",
    "    # on main diagonal compared to the actual result should be provided.\n",
    "    i_cnt = confusion_matrix_temp.shape[0]\n",
    "    j_cnt = confusion_matrix_temp.shape[1]\n",
    "    return ((confusion_matrix_temp*np.eye(i_cnt)).sum()) / (confusion_matrix_temp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb24d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_scoring(model, test_X, test_y, RMSE=True):\n",
    "### Write several scores to the output ###\n",
    "    print(model.estimator.named_steps)\n",
    "    print('Accuracy Score: ', sklearn.metrics.accuracy_score(test_y, model.predict(test_X)))\n",
    "    print('Balanced Accuracy Score: ', sklearn.metrics.balanced_accuracy_score(test_y, model.predict(test_X)))\n",
    "    print('Cohen-Kappa-Score: ', sklearn.metrics.cohen_kappa_score(test_y, model.predict(test_X)))\n",
    "    print('F1-Score: ', sklearn.metrics.f1_score(test_y, model.predict(test_X), average='weighted'))\n",
    "    if RMSE == True:\n",
    "        print('RMSE: ', np.sqrt(sklearn.metrics.mean_squared_error(test_y, model.predict(test_X))))\n",
    "        #print('ROC-AUC-Score: ', sklearn.metrics.roc_auc_score(test_y, \n",
    "        #                                                       model.predict(test_X), \n",
    "        #                                                       average='weighted', \n",
    "        #                                                       multi_class='ovo'))\n",
    "    print('__________________________________\\n')\n",
    "    print('Confusion matrix: \\n', sklearn.metrics.confusion_matrix(test_y, model.predict(test_X)))\n",
    "    print(confusion_matrix_relaxed(sklearn.metrics.confusion_matrix(test_y, model.predict(test_X))))    \n",
    "    print(confusion_matrix_perc(sklearn.metrics.confusion_matrix(test_y, model.predict(test_X))))\n",
    "    print(sklearn.metrics.classification_report(test_y, model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f54c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX = '_MIN_FOUR_VARIABLES_DATASET_NR_'#'_recategorized'\n",
    "FILE_TO_LOAD = 'CHES2019_experts.csv' # 'CHES2019_experts_recategorized_4l3c4r.csv' #' CHES2019_experts_recategorized.csv'#'CHES2019_experts.csv', \n",
    "FOLDER_PREFIX = '' #'base_data/' #'recategorized_4l3c4r/' # \n",
    "FOLDER_IMPUTE_PREFIX = 'imputed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e3f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'data_lrgen_base_'\n",
    "X_train_lrgen_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_train_lrgen_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "X_valid_lrgen_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_valid_lrgen_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "X_test_lrgen_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_test_lrgen_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrgen_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_train_lrgen_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "X_valid_lrgen_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_valid_lrgen_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "X_test_lrgen_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_test_lrgen_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrgen_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_train_lrgen_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "X_valid_lrgen_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_valid_lrgen_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "X_test_lrgen_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_test_lrgen_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrgen_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_train_lrgen_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "X_valid_lrgen_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_valid_lrgen_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "X_test_lrgen_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_test_lrgen_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrgen_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_train_lrgen_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "X_valid_lrgen_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_valid_lrgen_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "X_test_lrgen_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_test_lrgen_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(4) + '.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8be246",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'data_lrecon_base_'\n",
    "X_train_lrecon_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_train_lrecon_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "X_valid_lrecon_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_valid_lrecon_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "X_test_lrecon_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_test_lrecon_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrecon_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_train_lrecon_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "X_valid_lrecon_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_valid_lrecon_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "X_test_lrecon_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_test_lrecon_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrecon_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_train_lrecon_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "X_valid_lrecon_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_valid_lrecon_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "X_test_lrecon_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_test_lrecon_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrecon_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_train_lrecon_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "X_valid_lrecon_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_valid_lrecon_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "X_test_lrecon_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_test_lrecon_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "\n",
    "X_train_lrecon_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_train_lrecon_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "X_valid_lrecon_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_valid_lrecon_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "X_test_lrecon_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_test_lrecon_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(4) + '.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18658351",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'data_galtan_base_'\n",
    "X_train_galtan_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_train_galtan_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "X_valid_galtan_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_valid_galtan_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "X_test_galtan_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "y_test_galtan_0 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(0) + '.csv', index_col=0)\n",
    "\n",
    "X_train_galtan_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_train_galtan_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "X_valid_galtan_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_valid_galtan_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "X_test_galtan_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "y_test_galtan_1 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(1) + '.csv', index_col=0)\n",
    "\n",
    "X_train_galtan_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_train_galtan_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "X_valid_galtan_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_valid_galtan_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "X_test_galtan_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "y_test_galtan_2 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(2) + '.csv', index_col=0)\n",
    "\n",
    "X_train_galtan_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_train_galtan_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "X_valid_galtan_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_valid_galtan_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "X_test_galtan_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "y_test_galtan_3 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(3) + '.csv', index_col=0)\n",
    "\n",
    "X_train_galtan_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_train' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_train_galtan_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_train' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "X_valid_galtan_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_valid' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_valid_galtan_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_valid' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "X_test_galtan_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'X_test' + SUFFIX + str(4) + '.csv', index_col=0)\n",
    "y_test_galtan_4 = pd.read_csv('../data/base_data/' + FOLDER_PREFIX + FOLDER_IMPUTE_PREFIX + prefix + 'y_test' + SUFFIX + str(4) + '.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdeb99",
   "metadata": {},
   "source": [
    "# Train model with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d183e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### oversampling required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c019c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### PARAMETERS \n",
    "scoring_method = 'balanced_accuracy'#['balanced_accuracy', 'f1_samples', 'roc_auc_ovo_weighted']\n",
    "cv_nr = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b179704",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create related data collection for iteration\n",
    "data_temp = [[X_train_lrgen_0, X_valid_lrgen_0,y_train_lrgen_0, y_valid_lrgen_0, X_test_lrgen_0, y_test_lrgen_0],\n",
    "             [X_train_lrgen_1, X_valid_lrgen_1,y_train_lrgen_1, y_valid_lrgen_1, X_test_lrgen_1, y_test_lrgen_1],\n",
    "             [X_train_lrgen_2, X_valid_lrgen_2,y_train_lrgen_2, y_valid_lrgen_2, X_test_lrgen_2, y_test_lrgen_2],\n",
    "             [X_train_lrgen_3, X_valid_lrgen_3,y_train_lrgen_3, y_valid_lrgen_3, X_test_lrgen_3, y_test_lrgen_3],\n",
    "             [X_train_lrgen_4, X_valid_lrgen_4,y_train_lrgen_4, y_valid_lrgen_4, X_test_lrgen_4, y_test_lrgen_4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092cf9e",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ce5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid_svc = [{\n",
    "                    'svc__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__kernel': ['linear'],\n",
    "                    'svc__degree': [1,2,3,4,5]\n",
    "                  },\n",
    "                 {\n",
    "                    'svc__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__gamma': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "                    'svc__kernel': ['rbf']\n",
    "                 }]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9a4beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data\n",
    "#X_train = pd.concat([X_train_lrgen_0, X_valid_lrgen_0])\n",
    "#y_train = pd.concat([y_train_lrgen_0, y_valid_lrgen_0])\n",
    "#test_fold = np.full(len(X_train), -1)\n",
    "#test_fold[-len(X_valid_lrgen_0):] = 1\n",
    "#ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c826f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC dataset number 0\n",
      "0.40286238850893774\n",
      "{'svc__C': 10.0, 'svc__gamma': 0.05, 'svc__kernel': 'rbf'}\n",
      "Test accuracy: 0.360\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=10.0, gamma=0.05, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'svc': SVC(random_state=1)}\n",
      "Accuracy Score:  0.35988200589970504\n",
      "Balanced Accuracy Score:  0.35613139992137377\n",
      "Cohen-Kappa-Score:  0.2852965179542982\n",
      "F1-Score:  0.35548949541947106\n",
      "RMSE:  1.4644257484971224\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 1  5  1  1  0  0  0  0  0  0  0]\n",
      " [ 0 14  5  3  1  0  0  0  0  1  0]\n",
      " [ 0  3 14  8  2  1  0  0  1  1  0]\n",
      " [ 0  2  7 11  8  1  2  0  1  0  1]\n",
      " [ 0  0  3  5 14 10  5  1  0  0  0]\n",
      " [ 0  1  2  1 12 17  7  6  0  0  1]\n",
      " [ 0  0  0  0  5 11  6 11  6  0  0]\n",
      " [ 0  0  0  0  5  5  9 18  5  3  1]\n",
      " [ 0  0  0  0  0  0  0 14 13  6  3]\n",
      " [ 0  0  0  0  0  0  0  1  7  6  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  8  8]]\n",
      "0.43952802359882004\n",
      "0.35988200589970504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.12      0.22         8\n",
      "         1.0       0.56      0.58      0.57        24\n",
      "         2.0       0.44      0.47      0.45        30\n",
      "         3.0       0.38      0.33      0.35        33\n",
      "         4.0       0.30      0.37      0.33        38\n",
      "         5.0       0.38      0.36      0.37        47\n",
      "         6.0       0.21      0.15      0.18        39\n",
      "         7.0       0.35      0.39      0.37        46\n",
      "         8.0       0.39      0.36      0.38        36\n",
      "         9.0       0.24      0.27      0.26        22\n",
      "        10.0       0.36      0.50      0.42        16\n",
      "\n",
      "    accuracy                           0.36       339\n",
      "   macro avg       0.42      0.36      0.35       339\n",
      "weighted avg       0.37      0.36      0.36       339\n",
      "\n",
      "SVC dataset number 1\n",
      "0.35200681703785724\n",
      "{'svc__C': 10.0, 'svc__gamma': 0.5, 'svc__kernel': 'rbf'}\n",
      "Test accuracy: 0.366\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=10.0, gamma=0.5, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'svc': SVC(random_state=1)}\n",
      "Accuracy Score:  0.36578171091445427\n",
      "Balanced Accuracy Score:  0.3782595510897631\n",
      "Cohen-Kappa-Score:  0.2926670677975972\n",
      "F1-Score:  0.36527212300497863\n",
      "RMSE:  1.6311857532641454\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 2  2  2  0  0  0  0  0  0  0  0]\n",
      " [ 4  8  7  3  0  0  0  0  0  1  0]\n",
      " [ 1  2 11  9  1  4  0  0  0  0  0]\n",
      " [ 0  1  5 16  7  2  1  0  0  1  1]\n",
      " [ 0  0  3 11 17 11  2  2  0  1  1]\n",
      " [ 1  1  0  2  9 14 12  5  3  1  0]\n",
      " [ 0  2  0  3  5  6 10  7  1  0  0]\n",
      " [ 0  0  1  0  1  3  9 15 12  6  1]\n",
      " [ 0  1  0  0  1  1  4  7 13  3  1]\n",
      " [ 0  0  0  0  0  2  0  1  6  7  5]\n",
      " [ 0  0  0  0  0  0  0  0  2  5 11]]\n",
      "0.41002949852507375\n",
      "0.36578171091445427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.33      0.29         6\n",
      "         1.0       0.47      0.35      0.40        23\n",
      "         2.0       0.38      0.39      0.39        28\n",
      "         3.0       0.36      0.47      0.41        34\n",
      "         4.0       0.41      0.35      0.38        48\n",
      "         5.0       0.33      0.29      0.31        48\n",
      "         6.0       0.26      0.29      0.28        34\n",
      "         7.0       0.41      0.31      0.35        48\n",
      "         8.0       0.35      0.42      0.38        31\n",
      "         9.0       0.28      0.33      0.30        21\n",
      "        10.0       0.55      0.61      0.58        18\n",
      "\n",
      "    accuracy                           0.37       339\n",
      "   macro avg       0.37      0.38      0.37       339\n",
      "weighted avg       0.37      0.37      0.37       339\n",
      "\n",
      "SVC dataset number 2\n",
      "0.37379613213606144\n",
      "{'svc__C': 10.0, 'svc__gamma': 0.5, 'svc__kernel': 'rbf'}\n",
      "Test accuracy: 0.333\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=10.0, gamma=0.5, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'svc': SVC(random_state=1)}\n",
      "Accuracy Score:  0.3333333333333333\n",
      "Balanced Accuracy Score:  0.34241109348998927\n",
      "Cohen-Kappa-Score:  0.2547566242558654\n",
      "F1-Score:  0.3319586523081726\n",
      "RMSE:  1.95147625837996\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 2  2  1  0  0  0  1  0  0  0  1]\n",
      " [ 2  6  8  0  1  0  0  0  0  1  1]\n",
      " [ 0  1 12  8  3  2  1  0  0  0  0]\n",
      " [ 0  1  9  8 12  4  0  1  1  1  0]\n",
      " [ 1  1  2 12 16 10  6  3  0  0  0]\n",
      " [ 0  0  1  2  7 14 12  4  2  0  0]\n",
      " [ 0  0  1  0  4  8 14  8  3  1  0]\n",
      " [ 0  0  0  1  2  3  8 13  6  1  1]\n",
      " [ 1  0  2  0  2  1  5  8 12  9  3]\n",
      " [ 0  1  0  0  0  0  2  2  8  8  2]\n",
      " [ 0  2  0  0  0  0  0  0  0  6  8]]\n",
      "0.4306784660766962\n",
      "0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.29      0.31         7\n",
      "         1.0       0.43      0.32      0.36        19\n",
      "         2.0       0.33      0.44      0.38        27\n",
      "         3.0       0.26      0.22      0.24        37\n",
      "         4.0       0.34      0.31      0.33        51\n",
      "         5.0       0.33      0.33      0.33        42\n",
      "         6.0       0.29      0.36      0.32        39\n",
      "         7.0       0.33      0.37      0.35        35\n",
      "         8.0       0.38      0.28      0.32        43\n",
      "         9.0       0.30      0.35      0.32        23\n",
      "        10.0       0.50      0.50      0.50        16\n",
      "\n",
      "    accuracy                           0.33       339\n",
      "   macro avg       0.35      0.34      0.34       339\n",
      "weighted avg       0.34      0.33      0.33       339\n",
      "\n",
      "SVC dataset number 3\n",
      "0.3646825771040962\n",
      "{'svc__C': 10.0, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}\n",
      "Test accuracy: 0.383\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=10.0, gamma=0.1, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'svc': SVC(random_state=1)}\n",
      "Accuracy Score:  0.3834808259587021\n",
      "Balanced Accuracy Score:  0.37499240615291785\n",
      "Cohen-Kappa-Score:  0.3098816539229534\n",
      "F1-Score:  0.38146485096783644\n",
      "RMSE:  1.607504524553563\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 1  5  2  2  0  0  0  0  0  0  0]\n",
      " [ 0  7  7  0  4  0  0  0  0  0  1]\n",
      " [ 0  7 15  5  3  0  1  1  1  0  0]\n",
      " [ 0  2  4 12  6  6  0  0  1  0  1]\n",
      " [ 0  0  1  3 17 10  3  2  0  0  0]\n",
      " [ 0  0  1  0 13 21  9  5  2  1  0]\n",
      " [ 0  0  0  0  5 12 12  9  7  2  0]\n",
      " [ 0  0  2  0  5  4 10 13  4  1  0]\n",
      " [ 0  0  0  0  0  0  1 13 15  5  0]\n",
      " [ 0  0  0  0  0  0  0  1  5  9  3]\n",
      " [ 1  0  0  0  0  0  0  0  4  6  8]]\n",
      "0.40117994100294985\n",
      "0.3834808259587021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.10      0.17        10\n",
      "         1.0       0.33      0.37      0.35        19\n",
      "         2.0       0.47      0.45      0.46        33\n",
      "         3.0       0.55      0.38      0.44        32\n",
      "         4.0       0.32      0.47      0.38        36\n",
      "         5.0       0.40      0.40      0.40        52\n",
      "         6.0       0.33      0.26      0.29        47\n",
      "         7.0       0.30      0.33      0.31        39\n",
      "         8.0       0.38      0.44      0.41        34\n",
      "         9.0       0.38      0.50      0.43        18\n",
      "        10.0       0.62      0.42      0.50        19\n",
      "\n",
      "    accuracy                           0.38       339\n",
      "   macro avg       0.42      0.37      0.38       339\n",
      "weighted avg       0.40      0.38      0.38       339\n",
      "\n",
      "SVC dataset number 4\n",
      "0.4034752675936801\n",
      "{'svc__C': 10.0, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}\n",
      "Test accuracy: 0.327\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=10.0, gamma=0.1, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'svc': SVC(random_state=1)}\n",
      "Accuracy Score:  0.3274336283185841\n",
      "Balanced Accuracy Score:  0.33018965728625704\n",
      "Cohen-Kappa-Score:  0.24802988733874276\n",
      "F1-Score:  0.32485602088697957\n",
      "RMSE:  1.5853309863505431\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 2  4  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  5  9  3  0  0  0  0  0  0  0]\n",
      " [ 0  1 17  3  5  2  0  0  1  0  0]\n",
      " [ 0  1  7  7 10  2  2  0  0  0  1]\n",
      " [ 0  1  1 14 17 10  2  1  0  0  3]\n",
      " [ 0  0  2  2  7 11 15  4  0  0  1]\n",
      " [ 0  0  0  1  3  7 11 11  6  0  0]\n",
      " [ 0  0  1  0  0  5  7 15  8  3  1]\n",
      " [ 0  0  0  0  4  1  6 10 12  8  0]\n",
      " [ 0  0  0  0  0  0  0  2 11  4  7]\n",
      " [ 1  0  0  0  0  0  0  0  2  8 10]]\n",
      "0.46607669616519176\n",
      "0.3274336283185841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.33      0.40         6\n",
      "         1.0       0.42      0.28      0.33        18\n",
      "         2.0       0.46      0.59      0.52        29\n",
      "         3.0       0.23      0.23      0.23        30\n",
      "         4.0       0.37      0.35      0.36        49\n",
      "         5.0       0.29      0.26      0.28        42\n",
      "         6.0       0.26      0.28      0.27        39\n",
      "         7.0       0.35      0.38      0.36        40\n",
      "         8.0       0.30      0.29      0.30        41\n",
      "         9.0       0.17      0.17      0.17        24\n",
      "        10.0       0.43      0.48      0.45        21\n",
      "\n",
      "    accuracy                           0.33       339\n",
      "   macro avg       0.34      0.33      0.33       339\n",
      "weighted avg       0.33      0.33      0.32       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipelineSVC = make_pipeline(StandardScaler(), SVC(random_state=1))\n",
    "\n",
    "for i in range(0,5):\n",
    "    X_train = pd.concat([data_temp[i][0], data_temp[i][1]])\n",
    "    y_train = pd.concat([data_temp[i][2], data_temp[i][3]])\n",
    "    test_fold = np.full(len(X_train), -1)\n",
    "    test_fold[-len(data_temp[i][1]):] = 1\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    #print(ps.get_n_splits())\n",
    "    # Create an instance of GridSearch Cross-validation estimator\n",
    "    gsSVC = GridSearchCV(estimator=pipelineSVC,\n",
    "                         param_grid = param_grid_svc,\n",
    "                         scoring=scoring_method,\n",
    "                         cv=ps,\n",
    "                         refit=True,\n",
    "                         n_jobs=1)\n",
    "    # Train the SVM classifier\n",
    "    gsSVC.fit(X_train, np.ravel(y_train))\n",
    "    print('SVC dataset number ' + str(i))\n",
    "    # Print the training score of the best model\n",
    "    print(gsSVC.best_score_)\n",
    "    # Print the model parameters of the best model\n",
    "    print(gsSVC.best_params_)\n",
    "    # Print the model score on the test data using GridSearchCV score method\n",
    "    #print('Test accuracy: %.3f' % gsSVC.score(data_temp[i][4], data_temp[i][5]))\n",
    "    clfSVC = gsSVC.best_estimator_\n",
    "    print('Test accuracy: %.3f' % clfSVC.score(data_temp[i][4], data_temp[i][5]))\n",
    "    # Print the model score on the test data using Best estimator instance\n",
    "    #clfSVC = gsSVC.best_estimator_\n",
    "    print(clfSVC)\n",
    "    write_scoring(gsSVC, data_temp[i][4], data_temp[i][5], RMSE=True)\n",
    "    del(X_train, y_train, test_fold, ps, gsSVC, clfSVC)\n",
    "    \n",
    "    #print('Test accuracy: %.3f' % clfSVC.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12809e8",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f08b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineRFC = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=1, criterion='entropy')) \n",
    "# Create the parameter grid\n",
    "                            \n",
    "param_grid_rfc = [{\n",
    "    'randomforestclassifier__max_depth':[2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'randomforestclassifier__max_features':[2, 3, 4, 5, 6],\n",
    "    'randomforestclassifier__n_estimators':range(5, 105, 5)#,\n",
    "    #'randomforestclassifier__criterion:':['gini', 'entropy']\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bb0d31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC dataset number 0\n",
      "0.37230959028326327\n",
      "{'randomforestclassifier__max_depth': 9, 'randomforestclassifier__max_features': 2, 'randomforestclassifier__n_estimators': 60}\n",
      "Test accuracy: 0.330\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(criterion='entropy', max_depth=9,\n",
      "                                        max_features=2, n_estimators=60,\n",
      "                                        random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'randomforestclassifier': RandomForestClassifier(criterion='entropy', random_state=1)}\n",
      "Accuracy Score:  0.3303834808259587\n",
      "Balanced Accuracy Score:  0.33607164250048743\n",
      "Cohen-Kappa-Score:  0.25254968238242337\n",
      "F1-Score:  0.32923509232166487\n",
      "RMSE:  1.6411018674150117\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 2  3  2  0  0  1  0  0  0  0  0]\n",
      " [ 3  9  6  2  3  0  0  0  0  0  1]\n",
      " [ 0  5 13  6  1  3  1  0  0  0  1]\n",
      " [ 1  0  9 10  7  1  2  2  1  0  0]\n",
      " [ 0  0  4  4 13 13  3  1  0  0  0]\n",
      " [ 0  0  2  3  7 18 10  6  0  0  1]\n",
      " [ 0  0  0  0  3 14  6  9  4  3  0]\n",
      " [ 0  1  0  1  4  7  9 14  6  4  0]\n",
      " [ 0  0  1  0  0  0  0 12 11 11  1]\n",
      " [ 0  0  1  0  0  1  0  1  6  9  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  9  7]]\n",
      "0.45132743362831856\n",
      "0.3303834808259587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.25      0.29         8\n",
      "         1.0       0.50      0.38      0.43        24\n",
      "         2.0       0.34      0.43      0.38        30\n",
      "         3.0       0.38      0.30      0.34        33\n",
      "         4.0       0.34      0.34      0.34        38\n",
      "         5.0       0.31      0.38      0.34        47\n",
      "         6.0       0.19      0.15      0.17        39\n",
      "         7.0       0.31      0.30      0.31        46\n",
      "         8.0       0.39      0.31      0.34        36\n",
      "         9.0       0.25      0.41      0.31        22\n",
      "        10.0       0.47      0.44      0.45        16\n",
      "\n",
      "    accuracy                           0.33       339\n",
      "   macro avg       0.35      0.34      0.34       339\n",
      "weighted avg       0.34      0.33      0.33       339\n",
      "\n",
      "RFC dataset number 1\n",
      "0.37463397058924197\n",
      "{'randomforestclassifier__max_depth': 9, 'randomforestclassifier__max_features': 2, 'randomforestclassifier__n_estimators': 85}\n",
      "Test accuracy: 0.407\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(criterion='entropy', max_depth=9,\n",
      "                                        max_features=2, n_estimators=85,\n",
      "                                        random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'randomforestclassifier': RandomForestClassifier(criterion='entropy', random_state=1)}\n",
      "Accuracy Score:  0.40707964601769914\n",
      "Balanced Accuracy Score:  0.4229952223568674\n",
      "Cohen-Kappa-Score:  0.3364398609366326\n",
      "F1-Score:  0.40605365835178964\n",
      "RMSE:  1.6616443503056406\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 3  2  0  0  0  0  0  0  1  0  0]\n",
      " [ 2  9 10  1  0  0  0  0  0  1  0]\n",
      " [ 1  3 10  9  3  2  0  0  0  0  0]\n",
      " [ 0  0  5 18  7  2  0  0  0  1  1]\n",
      " [ 0  0  2  9 18 12  3  2  0  1  1]\n",
      " [ 0  1  0  3 10 15  8  8  3  0  0]\n",
      " [ 0  2  0  1  3  8 13  7  0  0  0]\n",
      " [ 0  0  0  1  0  3 10 22  8  3  1]\n",
      " [ 0  1  1  1  0  0  2 12 11  2  1]\n",
      " [ 0  1  0  0  1  0  0  1  8  8  2]\n",
      " [ 0  0  0  0  0  0  1  0  0  6 11]]\n",
      "0.41297935103244837\n",
      "0.40707964601769914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.50      0.50         6\n",
      "         1.0       0.47      0.39      0.43        23\n",
      "         2.0       0.36      0.36      0.36        28\n",
      "         3.0       0.42      0.53      0.47        34\n",
      "         4.0       0.43      0.38      0.40        48\n",
      "         5.0       0.36      0.31      0.33        48\n",
      "         6.0       0.35      0.38      0.37        34\n",
      "         7.0       0.42      0.46      0.44        48\n",
      "         8.0       0.35      0.35      0.35        31\n",
      "         9.0       0.36      0.38      0.37        21\n",
      "        10.0       0.65      0.61      0.63        18\n",
      "\n",
      "    accuracy                           0.41       339\n",
      "   macro avg       0.43      0.42      0.42       339\n",
      "weighted avg       0.41      0.41      0.41       339\n",
      "\n",
      "RFC dataset number 2\n",
      "0.3848596447121623\n",
      "{'randomforestclassifier__max_depth': 8, 'randomforestclassifier__max_features': 2, 'randomforestclassifier__n_estimators': 85}\n",
      "Test accuracy: 0.395\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(criterion='entropy', max_depth=8,\n",
      "                                        max_features=2, n_estimators=85,\n",
      "                                        random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'randomforestclassifier': RandomForestClassifier(criterion='entropy', random_state=1)}\n",
      "Accuracy Score:  0.3952802359882006\n",
      "Balanced Accuracy Score:  0.3949662881231342\n",
      "Cohen-Kappa-Score:  0.32361013781826675\n",
      "F1-Score:  0.39689947407529835\n",
      "RMSE:  1.6411018674150117\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 2  3  0  0  1  0  0  0  0  0  1]\n",
      " [ 2  7  7  0  1  0  0  0  0  0  2]\n",
      " [ 0  2 12  6  2  3  1  1  0  0  0]\n",
      " [ 0  2  6 16  6  5  0  1  1  0  0]\n",
      " [ 0  1  0 10 24  9  2  3  2  0  0]\n",
      " [ 0  0  1  3  7 16 10  4  1  0  0]\n",
      " [ 1  0  1  0  2 10 14  9  2  0  0]\n",
      " [ 0  0  0  0  1  8  5 13  7  0  1]\n",
      " [ 0  0  1  0  1  2  2 10 13 12  2]\n",
      " [ 0  0  0  0  0  0  0  4  9  7  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  6 10]]\n",
      "0.41002949852507375\n",
      "0.3952802359882006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.29      0.33         7\n",
      "         1.0       0.47      0.37      0.41        19\n",
      "         2.0       0.43      0.44      0.44        27\n",
      "         3.0       0.46      0.43      0.44        37\n",
      "         4.0       0.53      0.47      0.50        51\n",
      "         5.0       0.30      0.38      0.34        42\n",
      "         6.0       0.41      0.36      0.38        39\n",
      "         7.0       0.29      0.37      0.33        35\n",
      "         8.0       0.37      0.30      0.33        43\n",
      "         9.0       0.28      0.30      0.29        23\n",
      "        10.0       0.53      0.62      0.57        16\n",
      "\n",
      "    accuracy                           0.40       339\n",
      "   macro avg       0.41      0.39      0.40       339\n",
      "weighted avg       0.40      0.40      0.40       339\n",
      "\n",
      "RFC dataset number 3\n",
      "0.37519138018565074\n",
      "{'randomforestclassifier__max_depth': 10, 'randomforestclassifier__max_features': 2, 'randomforestclassifier__n_estimators': 50}\n",
      "Test accuracy: 0.381\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(criterion='entropy', max_depth=10,\n",
      "                                        max_features=2, n_estimators=50,\n",
      "                                        random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'randomforestclassifier': RandomForestClassifier(criterion='entropy', random_state=1)}\n",
      "Accuracy Score:  0.3805309734513274\n",
      "Balanced Accuracy Score:  0.3863167150268701\n",
      "Cohen-Kappa-Score:  0.3071128241065172\n",
      "F1-Score:  0.3806840338949134\n",
      "RMSE:  1.5188102780686472\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 3  2  4  0  0  0  1  0  0  0  0]\n",
      " [ 3  6  6  1  3  0  0  0  0  0  0]\n",
      " [ 1  3 13  8  4  1  0  2  1  0  0]\n",
      " [ 1  0  8 10  7  4  1  0  0  0  1]\n",
      " [ 0  0  2  2 19  7  3  2  1  0  0]\n",
      " [ 0  0  1  0 11 17 12  8  2  1  0]\n",
      " [ 0  0  0  0  4 12 14  9  5  3  0]\n",
      " [ 0  0  0  1  5  3 12 13  2  2  1]\n",
      " [ 0  0  0  0  0  0  1 13 16  4  0]\n",
      " [ 0  1  0  0  0  0  0  1  5  8  3]\n",
      " [ 0  0  0  0  0  0  0  0  3  6 10]]\n",
      "0.39823008849557523\n",
      "0.3805309734513274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.30      0.33        10\n",
      "         1.0       0.50      0.32      0.39        19\n",
      "         2.0       0.38      0.39      0.39        33\n",
      "         3.0       0.45      0.31      0.37        32\n",
      "         4.0       0.36      0.53      0.43        36\n",
      "         5.0       0.39      0.33      0.35        52\n",
      "         6.0       0.32      0.30      0.31        47\n",
      "         7.0       0.27      0.33      0.30        39\n",
      "         8.0       0.46      0.47      0.46        34\n",
      "         9.0       0.33      0.44      0.38        18\n",
      "        10.0       0.67      0.53      0.59        19\n",
      "\n",
      "    accuracy                           0.38       339\n",
      "   macro avg       0.41      0.39      0.39       339\n",
      "weighted avg       0.39      0.38      0.38       339\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC dataset number 4\n",
      "0.383902687827189\n",
      "{'randomforestclassifier__max_depth': 9, 'randomforestclassifier__max_features': 2, 'randomforestclassifier__n_estimators': 40}\n",
      "Test accuracy: 0.345\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(criterion='entropy', max_depth=9,\n",
      "                                        max_features=2, n_estimators=40,\n",
      "                                        random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'randomforestclassifier': RandomForestClassifier(criterion='entropy', random_state=1)}\n",
      "Accuracy Score:  0.34513274336283184\n",
      "Balanced Accuracy Score:  0.355769998621134\n",
      "Cohen-Kappa-Score:  0.26833109724085624\n",
      "F1-Score:  0.34167380391664387\n",
      "RMSE:  1.687189280909679\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 3  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  7  8  2  1  0  0  0  0  0  0]\n",
      " [ 0  5 12  4  6  1  1  0  0  0  0]\n",
      " [ 1  1  5  7 12  1  1  1  0  0  1]\n",
      " [ 0  1  3 12 14 13  0  2  0  1  3]\n",
      " [ 0  0  2  4  4 19  8  3  1  0  1]\n",
      " [ 0  0  0  2  3  9  7 10  7  1  0]\n",
      " [ 0  0  0  0  1  3  7 18  6  4  1]\n",
      " [ 0  0  1  0  3  3  4  6 17  6  1]\n",
      " [ 0  0  0  0  0  0  0  3 11  4  6]\n",
      " [ 1  0  0  1  0  0  0  0  2  8  9]]\n",
      "0.4218289085545723\n",
      "0.34513274336283184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.50      0.55         6\n",
      "         1.0       0.41      0.39      0.40        18\n",
      "         2.0       0.39      0.41      0.40        29\n",
      "         3.0       0.22      0.23      0.23        30\n",
      "         4.0       0.32      0.29      0.30        49\n",
      "         5.0       0.39      0.45      0.42        42\n",
      "         6.0       0.25      0.18      0.21        39\n",
      "         7.0       0.42      0.45      0.43        40\n",
      "         8.0       0.39      0.41      0.40        41\n",
      "         9.0       0.17      0.17      0.17        24\n",
      "        10.0       0.41      0.43      0.42        21\n",
      "\n",
      "    accuracy                           0.35       339\n",
      "   macro avg       0.36      0.36      0.36       339\n",
      "weighted avg       0.34      0.35      0.34       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    X_train = pd.concat([data_temp[i][0], data_temp[i][1]])\n",
    "    y_train = pd.concat([data_temp[i][2], data_temp[i][3]])\n",
    "    test_fold = np.full(len(X_train), -1)\n",
    "    test_fold[-len(data_temp[i][1]):] = 1\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    #print(ps.get_n_splits())\n",
    "    # Create an instance of GridSearch Cross-validation estimator\n",
    "    gsRFC = GridSearchCV(estimator=pipelineRFC,\n",
    "                         param_grid = param_grid_rfc,\n",
    "                         scoring=scoring_method,\n",
    "                         cv=cv_nr,\n",
    "                         refit=True,\n",
    "                         n_jobs=1)\n",
    "    # Train the RandomForestClassifier\n",
    "    gsRFC = gsRFC.fit(X_train, np.ravel(y_train))\n",
    "    print('RFC dataset number ' + str(i))\n",
    "    # Print the training score of the best model\n",
    "    print(gsRFC.best_score_)\n",
    "    # Print the model parameters of the best model\n",
    "    print(gsRFC.best_params_)\n",
    "    # Print the model score on the test data using GridSearchCV score method\n",
    "    clfRFC = gsRFC.best_estimator_\n",
    "    print('Test accuracy: %.3f' % clfRFC.score(data_temp[i][4], data_temp[i][5]))\n",
    "    # Print the model score on the test data using Best estimator instance\n",
    "    print(clfRFC)\n",
    "    write_scoring(gsRFC, data_temp[i][4], data_temp[i][5], RMSE=True)\n",
    "    del(X_train, y_train, test_fold, ps, gsRFC, clfRFC)\n",
    "    \n",
    "    #print('Test accuracy: %.3f' % clfSVC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9763171",
   "metadata": {},
   "source": [
    "## Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1db6c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineLR = make_pipeline(StandardScaler(), LogisticRegression(random_state=1, penalty='l2', solver='lbfgs', max_iter=1000))\n",
    "# Create the parameter grid\n",
    "param_grid_lr = [{\n",
    "    'logisticregression__C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0],\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "554c65b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3467638478521728\n",
      "{'logisticregression__C': 10.0}\n",
      "Test accuracy: 0.360\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=10.0, max_iter=1000, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'logisticregression': LogisticRegression(max_iter=1000, random_state=1)}\n",
      "Accuracy Score:  0.35988200589970504\n",
      "Balanced Accuracy Score:  0.37032177022057683\n",
      "Cohen-Kappa-Score:  0.2856019111991609\n",
      "F1-Score:  0.34250418732048743\n",
      "RMSE:  1.5514766018273365\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 1  3  3  1  0  0  0  0  0  0  0]\n",
      " [ 1  9  9  1  3  0  0  0  0  1  0]\n",
      " [ 0  2 19  3  4  0  0  1  0  0  1]\n",
      " [ 0  1 10  8  8  4  1  0  0  1  0]\n",
      " [ 0  0  3  5 12 13  2  3  0  0  0]\n",
      " [ 0  2  1  3 10 20  4  4  2  0  1]\n",
      " [ 0  1  0  0  3 12  5 12  5  1  0]\n",
      " [ 0  0  0  0  2  8  7 16  8  3  2]\n",
      " [ 0  0  0  0  3  1  0 10 13  6  3]\n",
      " [ 0  0  0  0  0  0  1  2  3  4 12]\n",
      " [ 0  0  0  0  0  0  0  0  0  1 15]]\n",
      "0.41002949852507375\n",
      "0.35988200589970504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.12      0.20         8\n",
      "         1.0       0.50      0.38      0.43        24\n",
      "         2.0       0.42      0.63      0.51        30\n",
      "         3.0       0.38      0.24      0.30        33\n",
      "         4.0       0.27      0.32      0.29        38\n",
      "         5.0       0.34      0.43      0.38        47\n",
      "         6.0       0.25      0.13      0.17        39\n",
      "         7.0       0.33      0.35      0.34        46\n",
      "         8.0       0.42      0.36      0.39        36\n",
      "         9.0       0.24      0.18      0.21        22\n",
      "        10.0       0.44      0.94      0.60        16\n",
      "\n",
      "    accuracy                           0.36       339\n",
      "   macro avg       0.37      0.37      0.35       339\n",
      "weighted avg       0.35      0.36      0.34       339\n",
      "\n",
      "0.33798160939448046\n",
      "{'logisticregression__C': 1.0}\n",
      "Test accuracy: 0.372\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(max_iter=1000, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'logisticregression': LogisticRegression(max_iter=1000, random_state=1)}\n",
      "Accuracy Score:  0.37168141592920356\n",
      "Balanced Accuracy Score:  0.35590078183511065\n",
      "Cohen-Kappa-Score:  0.29665309462118417\n",
      "F1-Score:  0.3647213961309813\n",
      "RMSE:  1.6338961163960337\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 1  4  0  0  1  0  0  0  0  0  0]\n",
      " [ 2  6 10  1  1  2  0  0  0  0  1]\n",
      " [ 0  2 16  6  4  0  0  0  0  0  0]\n",
      " [ 0  0  8 13  4  7  0  0  0  0  2]\n",
      " [ 0  0  4  7 24  9  1  1  0  1  1]\n",
      " [ 0  1  2  1 13 15  8  4  3  1  0]\n",
      " [ 0  0  0  1  7  9 11  5  1  0  0]\n",
      " [ 0  0  0  0  0  6 12 13 13  3  1]\n",
      " [ 0  0  0  0  1  2  3  5 15  3  2]\n",
      " [ 0  2  0  0  0  0  0  4  6  3  6]\n",
      " [ 0  0  0  0  0  0  0  0  3  6  9]]\n",
      "0.40707964601769914\n",
      "0.37168141592920356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.17      0.22         6\n",
      "         1.0       0.40      0.26      0.32        23\n",
      "         2.0       0.40      0.57      0.47        28\n",
      "         3.0       0.45      0.38      0.41        34\n",
      "         4.0       0.44      0.50      0.47        48\n",
      "         5.0       0.30      0.31      0.31        48\n",
      "         6.0       0.31      0.32      0.32        34\n",
      "         7.0       0.41      0.27      0.33        48\n",
      "         8.0       0.37      0.48      0.42        31\n",
      "         9.0       0.18      0.14      0.16        21\n",
      "        10.0       0.41      0.50      0.45        18\n",
      "\n",
      "    accuracy                           0.37       339\n",
      "   macro avg       0.36      0.36      0.35       339\n",
      "weighted avg       0.37      0.37      0.36       339\n",
      "\n",
      "0.34706531379326394\n",
      "{'logisticregression__C': 10.0}\n",
      "Test accuracy: 0.336\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=10.0, max_iter=1000, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'logisticregression': LogisticRegression(max_iter=1000, random_state=1)}\n",
      "Accuracy Score:  0.336283185840708\n",
      "Balanced Accuracy Score:  0.36263751370709646\n",
      "Cohen-Kappa-Score:  0.25785924866458443\n",
      "F1-Score:  0.32792777956753155\n",
      "RMSE:  1.393198819211568\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 2  3  1  1  0  0  0  0  0  0  0]\n",
      " [ 1  5 10  1  1  0  0  0  0  1  0]\n",
      " [ 0  1 14  4  5  3  0  0  0  0  0]\n",
      " [ 0  2 10  9 11  4  0  0  1  0  0]\n",
      " [ 0  1  3 10 16 15  2  4  0  0  0]\n",
      " [ 0  1  0  3  8 17  7  5  1  0  0]\n",
      " [ 0  0  0  0  3 18  8  8  2  0  0]\n",
      " [ 0  0  0  0  0 10  5 12  7  0  1]\n",
      " [ 0  0  0  0  2  4  4 11 10  4  8]\n",
      " [ 0  0  0  0  0  0  1  2  6  7  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  2 14]]\n",
      "0.4365781710914454\n",
      "0.336283185840708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.29      0.40         7\n",
      "         1.0       0.38      0.26      0.31        19\n",
      "         2.0       0.37      0.52      0.43        27\n",
      "         3.0       0.32      0.24      0.28        37\n",
      "         4.0       0.35      0.31      0.33        51\n",
      "         5.0       0.24      0.40      0.30        42\n",
      "         6.0       0.30      0.21      0.24        39\n",
      "         7.0       0.29      0.34      0.31        35\n",
      "         8.0       0.37      0.23      0.29        43\n",
      "         9.0       0.50      0.30      0.38        23\n",
      "        10.0       0.47      0.88      0.61        16\n",
      "\n",
      "    accuracy                           0.34       339\n",
      "   macro avg       0.39      0.36      0.35       339\n",
      "weighted avg       0.35      0.34      0.33       339\n",
      "\n",
      "0.34575760609520834\n",
      "{'logisticregression__C': 10.0}\n",
      "Test accuracy: 0.348\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=10.0, max_iter=1000, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'logisticregression': LogisticRegression(max_iter=1000, random_state=1)}\n",
      "Accuracy Score:  0.3480825958702065\n",
      "Balanced Accuracy Score:  0.3393998039721462\n",
      "Cohen-Kappa-Score:  0.2697312629762846\n",
      "F1-Score:  0.33840062632243045\n",
      "RMSE:  1.536190745285137\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 1  4  2  1  1  0  1  0  0  0  0]\n",
      " [ 1  7  7  1  3  0  0  0  0  0  0]\n",
      " [ 0  5 15  7  3  0  1  1  1  0  0]\n",
      " [ 0  0  7  8  9  6  0  1  0  0  1]\n",
      " [ 0  0  3  2 14 13  1  3  0  0  0]\n",
      " [ 0  0  1  0 13 23  5  7  2  1  0]\n",
      " [ 0  0  0  0  5 16  8 11  3  2  2]\n",
      " [ 0  1  0  0  5  7  6 15  5  0  0]\n",
      " [ 0  0  0  0  1  0  1 16 11  3  2]\n",
      " [ 0  0  0  0  0  0  0  1  2  3 12]\n",
      " [ 0  0  0  0  1  0  0  0  0  5 13]]\n",
      "0.43952802359882004\n",
      "0.3480825958702065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.10      0.17        10\n",
      "         1.0       0.41      0.37      0.39        19\n",
      "         2.0       0.43      0.45      0.44        33\n",
      "         3.0       0.42      0.25      0.31        32\n",
      "         4.0       0.25      0.39      0.31        36\n",
      "         5.0       0.35      0.44      0.39        52\n",
      "         6.0       0.35      0.17      0.23        47\n",
      "         7.0       0.27      0.38      0.32        39\n",
      "         8.0       0.46      0.32      0.38        34\n",
      "         9.0       0.21      0.17      0.19        18\n",
      "        10.0       0.43      0.68      0.53        19\n",
      "\n",
      "    accuracy                           0.35       339\n",
      "   macro avg       0.37      0.34      0.33       339\n",
      "weighted avg       0.36      0.35      0.34       339\n",
      "\n",
      "0.34915819525485803\n",
      "{'logisticregression__C': 10.0}\n",
      "Test accuracy: 0.339\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=10.0, max_iter=1000, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'logisticregression': LogisticRegression(max_iter=1000, random_state=1)}\n",
      "Accuracy Score:  0.3392330383480826\n",
      "Balanced Accuracy Score:  0.34412178493939877\n",
      "Cohen-Kappa-Score:  0.26210535521674494\n",
      "F1-Score:  0.332421535935309\n",
      "RMSE:  1.5862610720781622\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 2  3  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  6  9  2  1  0  0  0  0  0  0]\n",
      " [ 0  3 14  5  5  2  0  0  0  0  0]\n",
      " [ 0  1  7  6  9  5  0  1  0  0  1]\n",
      " [ 0  0  3 11 18 13  0  1  0  0  3]\n",
      " [ 0  0  2  3  4 21  6  3  2  0  1]\n",
      " [ 0  1  0  1  0 13  6 11  6  1  0]\n",
      " [ 0  0  0  0  1  6  6 13  8  4  2]\n",
      " [ 0  0  0  0  1  8  3  9 11  6  3]\n",
      " [ 0  0  0  0  0  0  0  4  5  6  9]\n",
      " [ 0  1  0  0  0  0  0  0  1  7 12]]\n",
      "0.4247787610619469\n",
      "0.3392330383480826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.33      0.50         6\n",
      "         1.0       0.40      0.33      0.36        18\n",
      "         2.0       0.39      0.48      0.43        29\n",
      "         3.0       0.21      0.20      0.21        30\n",
      "         4.0       0.46      0.37      0.41        49\n",
      "         5.0       0.31      0.50      0.38        42\n",
      "         6.0       0.29      0.15      0.20        39\n",
      "         7.0       0.31      0.33      0.32        40\n",
      "         8.0       0.33      0.27      0.30        41\n",
      "         9.0       0.25      0.25      0.25        24\n",
      "        10.0       0.39      0.57      0.46        21\n",
      "\n",
      "    accuracy                           0.34       339\n",
      "   macro avg       0.39      0.34      0.35       339\n",
      "weighted avg       0.35      0.34      0.33       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    X_train = pd.concat([data_temp[i][0], data_temp[i][1]])\n",
    "    y_train = pd.concat([data_temp[i][2], data_temp[i][3]])\n",
    "    test_fold = np.full(len(X_train), -1)\n",
    "    test_fold[-len(data_temp[i][1]):] = 1\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    #print(ps.get_n_splits())\n",
    "    # Create an instance of GridSearch Cross-validation estimator\n",
    "    gsLR = GridSearchCV(estimator=pipelineLR,\n",
    "                         param_grid = param_grid_lr,\n",
    "                         scoring=scoring_method,\n",
    "                         cv=cv_nr,\n",
    "                         refit=True,\n",
    "                         n_jobs=1)\n",
    "    # Train the LogisticRegression Classifier\n",
    "    gsLR = gsLR.fit(X_train, np.ravel(y_train))\n",
    "    # Print the training score of the best model\n",
    "    print(gsLR.best_score_)\n",
    "    # Print the model parameters of the best model\n",
    "    print(gsLR.best_params_)\n",
    "    # Print the test score of the best model\n",
    "    clfLR = gsLR.best_estimator_\n",
    "    print('Test accuracy: %.3f' % clfLR.score(data_temp[i][4], data_temp[i][5]))\n",
    "    # Print the model score on the test data using Best estimator instance\n",
    "    print(clfLR)\n",
    "    write_scoring(gsLR, data_temp[i][4], data_temp[i][5], RMSE=True)\n",
    "    del(X_train, y_train, test_fold, ps, gsLR, clfLR)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae647387",
   "metadata": {},
   "source": [
    "## ADABoost Classification (Ensemble Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bba3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineADAB = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=1))\n",
    "# Create the parameter grid\n",
    "param_grid_ADAB = [{\n",
    "    'adaboostclassifier__n_estimators': [2, 10, 20, 30, 40, 50, 100]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "020a2f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31158810780664864\n",
      "{'adaboostclassifier__n_estimators': 20}\n",
      "Test accuracy: 0.242\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('adaboostclassifier',\n",
      "                 AdaBoostClassifier(n_estimators=20, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'adaboostclassifier': AdaBoostClassifier(random_state=1)}\n",
      "Accuracy Score:  0.24188790560471976\n",
      "Balanced Accuracy Score:  0.3017308723274978\n",
      "Cohen-Kappa-Score:  0.165616380631321\n",
      "F1-Score:  0.20520196027124787\n",
      "RMSE:  1.7599275930399418\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 6  0  0  2  0  0  0  0  0  0  0]\n",
      " [14  1  1  7  0  0  0  0  0  0  1]\n",
      " [14  0  1 12  0  1  0  1  0  1  0]\n",
      " [ 5  0  1 16  3  4  3  0  0  0  1]\n",
      " [ 0  0  1 13  8  7  8  0  0  1  0]\n",
      " [ 1  0  1  6 10  9 18  0  0  1  1]\n",
      " [ 0  0  0  4  2 10 17  4  0  2  0]\n",
      " [ 0  0  1  1  5  5 23  3  1  5  2]\n",
      " [ 0  1  0  0  0  0 10  3  3 14  5]\n",
      " [ 0  0  0  0  0  1  0  1  2  6 12]\n",
      " [ 0  0  0  0  0  0  0  0  0  4 12]]\n",
      "0.44837758112094395\n",
      "0.24188790560471976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.15      0.75      0.25         8\n",
      "         1.0       0.50      0.04      0.08        24\n",
      "         2.0       0.17      0.03      0.06        30\n",
      "         3.0       0.26      0.48      0.34        33\n",
      "         4.0       0.29      0.21      0.24        38\n",
      "         5.0       0.24      0.19      0.21        47\n",
      "         6.0       0.22      0.44      0.29        39\n",
      "         7.0       0.25      0.07      0.10        46\n",
      "         8.0       0.50      0.08      0.14        36\n",
      "         9.0       0.18      0.27      0.21        22\n",
      "        10.0       0.35      0.75      0.48        16\n",
      "\n",
      "    accuracy                           0.24       339\n",
      "   macro avg       0.28      0.30      0.22       339\n",
      "weighted avg       0.28      0.24      0.21       339\n",
      "\n",
      "0.2893716898582824\n",
      "{'adaboostclassifier__n_estimators': 20}\n",
      "Test accuracy: 0.242\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('adaboostclassifier',\n",
      "                 AdaBoostClassifier(n_estimators=20, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'adaboostclassifier': AdaBoostClassifier(random_state=1)}\n",
      "Accuracy Score:  0.24188790560471976\n",
      "Balanced Accuracy Score:  0.2862836483567034\n",
      "Cohen-Kappa-Score:  0.16169041731215195\n",
      "F1-Score:  0.2202588261710872\n",
      "RMSE:  1.9799883793348094\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 5  1  0  0  0  0  0  0  0  0  0]\n",
      " [17  0  0  3  0  1  1  0  0  0  1]\n",
      " [10  0  5  9  2  2  0  0  0  0  0]\n",
      " [ 4  0 14  5  4  4  1  0  0  0  2]\n",
      " [ 2  0 10  2 15 14  2  1  0  1  1]\n",
      " [ 1  2  4  0  4 24  8  2  2  1  0]\n",
      " [ 0  1  1  2  2 20  7  1  0  0  0]\n",
      " [ 0  0  1  0  0 17 15  2  5  4  4]\n",
      " [ 0  0  2  1  0  6 10  0  3  4  5]\n",
      " [ 0  0  0  0  0  0  2  0  3  7  9]\n",
      " [ 1  0  0  0  0  0  0  0  2  6  9]]\n",
      "0.40117994100294985\n",
      "0.24188790560471976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.12      0.83      0.22         6\n",
      "         1.0       0.00      0.00      0.00        23\n",
      "         2.0       0.14      0.18      0.15        28\n",
      "         3.0       0.23      0.15      0.18        34\n",
      "         4.0       0.56      0.31      0.40        48\n",
      "         5.0       0.27      0.50      0.35        48\n",
      "         6.0       0.15      0.21      0.17        34\n",
      "         7.0       0.33      0.04      0.07        48\n",
      "         8.0       0.20      0.10      0.13        31\n",
      "         9.0       0.30      0.33      0.32        21\n",
      "        10.0       0.29      0.50      0.37        18\n",
      "\n",
      "    accuracy                           0.24       339\n",
      "   macro avg       0.24      0.29      0.22       339\n",
      "weighted avg       0.27      0.24      0.22       339\n",
      "\n",
      "0.3089308806378387\n",
      "{'adaboostclassifier__n_estimators': 20}\n",
      "Test accuracy: 0.260\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('adaboostclassifier',\n",
      "                 AdaBoostClassifier(n_estimators=20, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'adaboostclassifier': AdaBoostClassifier(random_state=1)}\n",
      "Accuracy Score:  0.25958702064896755\n",
      "Balanced Accuracy Score:  0.2944944668243056\n",
      "Cohen-Kappa-Score:  0.17840797172817335\n",
      "F1-Score:  0.22372096674823191\n",
      "RMSE:  1.9286689741371923\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 4  0  0  0  0  1  0  0  0  0  2]\n",
      " [14  1  1  1  0  1  0  0  0  0  1]\n",
      " [12  0  0  7  4  1  3  0  0  0  0]\n",
      " [ 6  1  0  8 10  6  4  0  0  1  1]\n",
      " [ 1  0  0 10 17 10  8  4  1  0  0]\n",
      " [ 0  0  0  3  5  8 22  4  0  0  0]\n",
      " [ 0  1  1  0  1  8 25  0  2  1  0]\n",
      " [ 0  0  0  0  2  9 20  0  2  1  1]\n",
      " [ 1  0  0  0  0  1 17  3  4 10  7]\n",
      " [ 0  0  0  0  0  1  3  2  2  9  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  4 12]]\n",
      "0.3952802359882006\n",
      "0.25958702064896755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.11      0.57      0.18         7\n",
      "         1.0       0.33      0.05      0.09        19\n",
      "         2.0       0.00      0.00      0.00        27\n",
      "         3.0       0.28      0.22      0.24        37\n",
      "         4.0       0.44      0.33      0.38        51\n",
      "         5.0       0.17      0.19      0.18        42\n",
      "         6.0       0.25      0.64      0.35        39\n",
      "         7.0       0.00      0.00      0.00        35\n",
      "         8.0       0.36      0.09      0.15        43\n",
      "         9.0       0.35      0.39      0.37        23\n",
      "        10.0       0.40      0.75      0.52        16\n",
      "\n",
      "    accuracy                           0.26       339\n",
      "   macro avg       0.24      0.29      0.22       339\n",
      "weighted avg       0.25      0.26      0.22       339\n",
      "\n",
      "0.2970090492406995\n",
      "{'adaboostclassifier__n_estimators': 20}\n",
      "Test accuracy: 0.248\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('adaboostclassifier',\n",
      "                 AdaBoostClassifier(n_estimators=20, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'adaboostclassifier': AdaBoostClassifier(random_state=1)}\n",
      "Accuracy Score:  0.24778761061946902\n",
      "Balanced Accuracy Score:  0.2834826191052724\n",
      "Cohen-Kappa-Score:  0.16776578640814088\n",
      "F1-Score:  0.19785065780102382\n",
      "RMSE:  1.912542054215944\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 6  3  0  0  0  0  1  0  0  0  0]\n",
      " [12  3  0  3  1  0  0  0  0  0  0]\n",
      " [13  5  0  6  6  0  2  1  0  0  0]\n",
      " [ 2  5  0  6 12  1  4  1  0  0  1]\n",
      " [ 0  1  0  8 14  0  9  0  2  2  0]\n",
      " [ 0  3  0  2 13  2 25  3  4  0  0]\n",
      " [ 0  2  0  0  6  1 25  5  5  1  2]\n",
      " [ 0  3  0  2  7  1 17  2  4  1  2]\n",
      " [ 0  0  0  0  1  1  8  8  9  1  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  1 17]\n",
      " [ 0  1  0  0  0  0  0  1  1  0 16]]\n",
      "0.40412979351032446\n",
      "0.24778761061946902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.18      0.60      0.28        10\n",
      "         1.0       0.12      0.16      0.13        19\n",
      "         2.0       0.00      0.00      0.00        33\n",
      "         3.0       0.22      0.19      0.20        32\n",
      "         4.0       0.23      0.39      0.29        36\n",
      "         5.0       0.33      0.04      0.07        52\n",
      "         6.0       0.27      0.53      0.36        47\n",
      "         7.0       0.10      0.05      0.07        39\n",
      "         8.0       0.36      0.26      0.31        34\n",
      "         9.0       0.17      0.06      0.08        18\n",
      "        10.0       0.36      0.84      0.51        19\n",
      "\n",
      "    accuracy                           0.25       339\n",
      "   macro avg       0.21      0.28      0.21       339\n",
      "weighted avg       0.22      0.25      0.20       339\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gernot/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gernot/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gernot/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3031208361836166\n",
      "{'adaboostclassifier__n_estimators': 10}\n",
      "Test accuracy: 0.289\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('adaboostclassifier',\n",
      "                 AdaBoostClassifier(n_estimators=10, random_state=1))])\n",
      "{'standardscaler': StandardScaler(), 'adaboostclassifier': AdaBoostClassifier(random_state=1)}\n",
      "Accuracy Score:  0.2890855457227139\n",
      "Balanced Accuracy Score:  0.2642030583457954\n",
      "Cohen-Kappa-Score:  0.2061429931787706\n",
      "F1-Score:  0.25899153533098823\n",
      "RMSE:  1.848223255870303\n",
      "__________________________________\n",
      "\n",
      "Confusion matrix: \n",
      " [[ 0  0  0  6  0  0  0  0  0  0  0]\n",
      " [ 0  0  5 13  0  0  0  0  0  0  0]\n",
      " [ 0  0 13  9  4  1  1  0  0  1  0]\n",
      " [ 0  0 12  2 12  1  2  0  0  0  1]\n",
      " [ 0  2 10  3 20  2  9  0  0  0  3]\n",
      " [ 0  0  8  0  9  5 18  0  1  0  1]\n",
      " [ 0  0  4  1  2  2 25  2  1  2  0]\n",
      " [ 0  1  0  0  4  2 22  2  2  7  0]\n",
      " [ 0  0  2  0  5  0 11  1 11 11  0]\n",
      " [ 0  0  0  0  0  0  4  1  5  8  6]\n",
      " [ 0  0  0  0  0  0  0  0  2  7 12]]\n",
      "0.3775811209439528\n",
      "0.2890855457227139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         6\n",
      "         1.0       0.00      0.00      0.00        18\n",
      "         2.0       0.24      0.45      0.31        29\n",
      "         3.0       0.06      0.07      0.06        30\n",
      "         4.0       0.36      0.41      0.38        49\n",
      "         5.0       0.38      0.12      0.18        42\n",
      "         6.0       0.27      0.64      0.38        39\n",
      "         7.0       0.33      0.05      0.09        40\n",
      "         8.0       0.50      0.27      0.35        41\n",
      "         9.0       0.22      0.33      0.27        24\n",
      "        10.0       0.52      0.57      0.55        21\n",
      "\n",
      "    accuracy                           0.29       339\n",
      "   macro avg       0.26      0.26      0.23       339\n",
      "weighted avg       0.30      0.29      0.26       339\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gernot/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gernot/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gernot/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    X_train = pd.concat([data_temp[i][0], data_temp[i][1]])\n",
    "    y_train = pd.concat([data_temp[i][2], data_temp[i][3]])\n",
    "    test_fold = np.full(len(X_train), -1)\n",
    "    test_fold[-len(data_temp[i][1]):] = 1\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    # Create an instance of GridSearch Cross-validation estimator\n",
    "    gsADAB = GridSearchCV(estimator=pipelineADAB,\n",
    "                         param_grid = param_grid_ADAB,\n",
    "                         scoring=scoring_method,\n",
    "                         cv=cv_nr,\n",
    "                         refit=True,\n",
    "                         n_jobs=1)\n",
    "    # Train the LogisticRegression Classifier\n",
    "    gsADAB = gsADAB.fit(X_train, np.ravel(y_train))\n",
    "    # Print the training score of the best model\n",
    "    print(gsADAB.best_score_)\n",
    "    # Print the model parameters of the best model\n",
    "    print(gsADAB.best_params_)\n",
    "    # Print the test score of the best model\n",
    "    clfADAB = gsADAB.best_estimator_\n",
    "    print('Test accuracy: %.3f' % clfADAB.score(data_temp[i][4], data_temp[i][5]))\n",
    "    # Print the model score on the test data using Best estimator instance\n",
    "    print(clfADAB)\n",
    "    write_scoring(gsADAB, data_temp[i][4], data_temp[i][5], RMSE=True)\n",
    "    del(X_train, y_train, test_fold, ps, gsADAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5b15e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accec0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f6810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53a1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a2f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21c608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702989b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4157a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
